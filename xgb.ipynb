{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBm2QpLhj7+BCs8YX5uRfH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saiprathyushadike/ML_DL_MODELS_IMPLIMENTATION_BY_ME/blob/main/xgb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "575e6cd8"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class Node:\n",
        "    \"\"\"Represents a node in the decision tree.\"\"\"\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None, sum_grad=None, sum_hess=None, missing_direction=None):\n",
        "        self.feature_index = feature_index  # Index of the feature to split on\n",
        "        self.threshold = threshold          # Threshold value for the split\n",
        "        self.left = left                    # Left child node\n",
        "        self.right = right                  # Right child node\n",
        "        self.value = value                  # Predicted value if this is a leaf node\n",
        "        self.sum_grad = sum_grad            # Sum of gradients for data points in this node\n",
        "        self.sum_hess = sum_hess            # Sum of Hessians for data points in this node\n",
        "        self.missing_direction = missing_direction # Direction for missing values ('left' or 'right')\n",
        "\n",
        "class DecisionTreeRegressor:\n",
        "    \"\"\"Basic implementation of a Decision Tree Regressor with regularization and missing value handling.\"\"\"\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, reg_lambda=0, reg_gamma=0):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda # L2 regularization term\n",
        "        self.reg_gamma = reg_gamma   # Regularization term on the number of leaves\n",
        "        self.root = None\n",
        "\n",
        "    def _calculate_gain(self, G_L, H_L, G_R, H_R):\n",
        "        \"\"\"Calculates the gain for a split with regularization.\"\"\"\n",
        "        # Avoid division by zero by adding a small epsilon or checking H_L + H_R + self.reg_lambda\n",
        "        gain = 0.5 * (G_L**2 / (H_L + self.reg_lambda) + G_R**2 / (H_R + self.reg_lambda) - (G_L + G_R)**2 / (H_L + H_R + self.reg_lambda + 1e-6)) - self.reg_gamma\n",
        "        return gain\n",
        "\n",
        "    def _find_best_split(self, X, gradients, hessians):\n",
        "        \"\"\"Finds the best feature and threshold to split the data considering gain with regularization and missing values.\"\"\"\n",
        "        best_gain = -float('inf')\n",
        "        best_feature_index = None\n",
        "        best_threshold = None\n",
        "        best_missing_direction = None\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        G = np.sum(gradients)\n",
        "        H = np.sum(hessians)\n",
        "\n",
        "        for feature_index in range(n_features):\n",
        "            # Handle missing values: separate non-missing and missing indices\n",
        "            non_missing_indices = ~np.isnan(X[:, feature_index])\n",
        "            missing_indices = np.isnan(X[:, feature_index])\n",
        "\n",
        "            X_non_missing = X[non_missing_indices, feature_index]\n",
        "            gradients_non_missing = gradients[non_missing_indices]\n",
        "            hessians_non_missing = hessians[non_missing_indices]\n",
        "\n",
        "            G_missing = np.sum(gradients[missing_indices])\n",
        "            H_missing = np.sum(hessians[missing_indices])\n",
        "\n",
        "            if len(X_non_missing) < self.min_samples_split:\n",
        "                continue # Not enough non-missing samples to split\n",
        "\n",
        "            # Get unique values for thresholds, potentially sort them\n",
        "            thresholds = np.unique(X_non_missing)\n",
        "            # Consider sorting thresholds for potentially better performance/consistency\n",
        "            # thresholds = np.sort(thresholds)\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                # Consider splitting non-missing values\n",
        "                left_indices_non_missing = X_non_missing <= threshold\n",
        "                right_indices_non_missing = X_non_missing > threshold\n",
        "\n",
        "                G_L_non_missing = np.sum(gradients_non_missing[left_indices_non_missing])\n",
        "                H_L_non_missing = np.sum(hessians_non_missing[left_indices_non_missing])\n",
        "                G_R_non_missing = np.sum(gradients_non_missing[right_indices_non_missing])\n",
        "                H_R_non_missing = np.sum(hessians_non_missing[right_indices_non_missing])\n",
        "\n",
        "                # Explore sending missing values to the left child\n",
        "                G_L_missing_left = G_L_non_missing + G_missing\n",
        "                H_L_missing_left = H_L_non_missing + H_missing\n",
        "                G_R_missing_left = G_R_non_missing\n",
        "                H_R_missing_left = H_R_non_missing\n",
        "                gain_missing_left = self._calculate_gain(G_L_missing_left, H_L_missing_left, G_R_missing_left, H_R_missing_left)\n",
        "\n",
        "                # Explore sending missing values to the right child\n",
        "                G_L_missing_right = G_L_non_missing\n",
        "                H_L_missing_right = H_L_non_missing\n",
        "                G_R_missing_right = G_R_non_missing + G_missing\n",
        "                H_R_missing_right = H_R_non_missing + H_missing\n",
        "                gain_missing_right = self._calculate_gain(G_L_missing_right, H_L_missing_right, G_R_missing_right, H_R_missing_right)\n",
        "\n",
        "\n",
        "                # Compare gains and update best split\n",
        "                if gain_missing_left > best_gain:\n",
        "                    best_gain = gain_missing_left\n",
        "                    best_feature_index = feature_index\n",
        "                    best_threshold = threshold\n",
        "                    best_missing_direction = 'left'\n",
        "\n",
        "                if gain_missing_right > best_gain:\n",
        "                    best_gain = gain_missing_right\n",
        "                    best_feature_index = feature_index\n",
        "                    best_threshold = threshold\n",
        "                    best_missing_direction = 'right'\n",
        "\n",
        "        # Only split if the best gain is positive (worth splitting)\n",
        "        if best_gain <= 0:\n",
        "             return None, None, None\n",
        "\n",
        "        return best_feature_index, best_threshold, best_missing_direction\n",
        "\n",
        "    def _build_tree(self, X, gradients, hessians, depth):\n",
        "        \"\"\"Recursively builds the decision tree with regularization and missing value handling.\"\"\"\n",
        "        n_samples = len(gradients)\n",
        "        sum_grad = np.sum(gradients)\n",
        "        sum_hess = np.sum(hessians)\n",
        "\n",
        "        # Stopping criteria\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
        "           (n_samples < self.min_samples_split) or \\\n",
        "           (sum_hess + self.reg_lambda == 0): # Avoid division by zero when calculating leaf value\n",
        "\n",
        "            # Calculate optimal leaf value with regularization\n",
        "            leaf_value = -sum_grad / (sum_hess + self.reg_lambda) if sum_hess + self.reg_lambda != 0 else 0\n",
        "            return Node(value=leaf_value, sum_grad=sum_grad, sum_hess=sum_hess)\n",
        "\n",
        "        best_feature_index, best_threshold, missing_direction = self._find_best_split(X, gradients, hessians)\n",
        "\n",
        "        # If no good split is found or gain is not positive, create a leaf node\n",
        "        if best_feature_index is None:\n",
        "            leaf_value = -sum_grad / (sum_hess + self.reg_lambda) if sum_hess + self.reg_lambda != 0 else 0\n",
        "            return Node(value=leaf_value, sum_grad=sum_grad, sum_hess=sum_hess)\n",
        "\n",
        "        # Split the data based on the best split and missing direction\n",
        "        feature_values = X[:, best_feature_index]\n",
        "        left_indices = (feature_values <= best_threshold) & (~np.isnan(feature_values))\n",
        "        right_indices = (feature_values > best_threshold) & (~np.isnan(feature_values))\n",
        "\n",
        "        if missing_direction == 'left':\n",
        "            left_indices = left_indices | np.isnan(feature_values)\n",
        "        else: # missing_direction == 'right'\n",
        "            right_indices = right_indices | np.isnan(feature_values)\n",
        "\n",
        "        X_left, gradients_left, hessians_left = X[left_indices], gradients[left_indices], hessians[left_indices]\n",
        "        X_right, gradients_right, hessians_right = X[right_indices], gradients[right_indices], hessians[right_indices]\n",
        "\n",
        "        # Ensure that after splitting, at least one sample goes to each child\n",
        "        if len(X_left) == 0 or len(X_right) == 0:\n",
        "             leaf_value = -sum_grad / (sum_hess + self.reg_lambda) if sum_hess + self.reg_lambda != 0 else 0\n",
        "             return Node(value=leaf_value, sum_grad=sum_grad, sum_hess=sum_hess)\n",
        "\n",
        "\n",
        "        # Recursively build subtrees\n",
        "        left_subtree = self._build_tree(X_left, gradients_left, hessians_left, depth + 1)\n",
        "        right_subtree = self._build_tree(X_right, gradients_right, hessians_right, depth + 1)\n",
        "\n",
        "        return Node(feature_index=best_feature_index, threshold=best_threshold,\n",
        "                    left=left_subtree, right=right_subtree, sum_grad=sum_grad,\n",
        "                    sum_hess=sum_hess, missing_direction=missing_direction)\n",
        "\n",
        "\n",
        "    def fit(self, X, gradients, hessians):\n",
        "        \"\"\"Builds the decision tree using gradients and hessians.\"\"\"\n",
        "        self.root = self._build_tree(X, gradients, hessians, depth=0)\n",
        "\n",
        "    def _predict_single(self, x, node):\n",
        "        \"\"\"Predicts the value for a single data point by traversing the tree, handling missing values.\"\"\"\n",
        "        if node is None: # Should not happen in a properly built tree, but as a safeguard\n",
        "            return 0 # Or some default value\n",
        "\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        feature_value = x[node.feature_index]\n",
        "\n",
        "        if np.isnan(feature_value):\n",
        "            if node.missing_direction == 'left':\n",
        "                return self._predict_single(x, node.left)\n",
        "            else: # missing_direction == 'right'\n",
        "                return self._predict_single(x, node.right)\n",
        "        elif feature_value <= node.threshold:\n",
        "            return self._predict_single(x, node.left)\n",
        "        else:\n",
        "            return self._predict_single(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts values for a set of data points.\"\"\"\n",
        "        return np.array([self._predict_single(x, self.root) for x in X])\n",
        "\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    \"\"\"Implements the Gradient Boosting algorithm from scratch with regularization, missing value handling, and early stopping.\"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, loss='mse', max_depth=3, reg_lambda=0, reg_gamma=0, subsample=1.0, colsample_bytree=1.0, validation_fraction=0.1, n_iter_no_change=None, tol=1e-4):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = loss\n",
        "        self.max_depth = max_depth\n",
        "        self.reg_lambda = reg_lambda # L2 regularization term\n",
        "        self.reg_gamma = reg_gamma   # Regularization term on the number of leaves\n",
        "        self.subsample = subsample\n",
        "        self.colsample_bytree = colsample_bytree\n",
        "        self.validation_fraction = validation_fraction\n",
        "        self.n_iter_no_change = n_iter_no_change # For early stopping\n",
        "        self.tol = tol # Tolerance for early stopping\n",
        "        self.estimators = []\n",
        "        self.initial_prediction = None\n",
        "        self.evals_result = {} # To store evaluation results for early stopping\n",
        "\n",
        "        if self.loss == 'mse':\n",
        "            self._gradient = self._mse_gradient\n",
        "            self._hessian = self._mse_hessian\n",
        "            self._eval_metric = lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)) # RMSE for evaluation\n",
        "        else:\n",
        "            raise ValueError(f\"Loss function '{self.loss}' not supported.\")\n",
        "\n",
        "    def _mse_gradient(self, y_true, y_pred):\n",
        "        \"\"\"Calculates the gradient (first derivative) for MSE loss.\"\"\"\n",
        "        return -(y_true - y_pred)\n",
        "\n",
        "    def _mse_hessian(self, y_true, y_pred):\n",
        "        \"\"\"Calculates the Hessian (second derivative) for MSE loss.\"\"\"\n",
        "        return np.ones_like(y_true)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fits the gradient boosting model to the training data with early stopping.\"\"\"\n",
        "        n_samples = len(y)\n",
        "\n",
        "        # Split data for early stopping if validation_fraction is set\n",
        "        if self.n_iter_no_change is not None and self.validation_fraction > 0:\n",
        "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.validation_fraction, random_state=42)\n",
        "        else:\n",
        "            X_train, y_train = X, y\n",
        "            X_val, y_val = None, None\n",
        "\n",
        "        # 1. Initialize the predictions (e.g., with the mean of y for regression)\n",
        "        self.initial_prediction = np.mean(y_train)\n",
        "        y_pred_train = np.full_like(y_train, self.initial_prediction, dtype=float)\n",
        "        y_pred_val = None\n",
        "        if X_val is not None:\n",
        "             y_pred_val = np.full_like(y_val, self.initial_prediction, dtype=float)\n",
        "\n",
        "        best_val_score = float('inf')\n",
        "        no_change_count = 0\n",
        "\n",
        "        # 2. Iteratively train base learners\n",
        "        for i in range(self.n_estimators):\n",
        "            # Calculate the negative gradient (pseudo-residuals) and hessians for the training data\n",
        "            gradients = self._gradient(y_train, y_pred_train)\n",
        "            hessians = self._hessian(y_train, y_pred_train)\n",
        "\n",
        "            # Apply subsampling and column subsampling\n",
        "            sample_indices = np.random.choice(len(X_train), max(1, int(len(X_train) * self.subsample)), replace=False)\n",
        "            feature_indices = np.random.choice(X_train.shape[1], max(1, int(X_train.shape[1] * self.colsample_bytree)), replace=False)\n",
        "\n",
        "            X_train_subset = X_train[sample_indices][:, feature_indices]\n",
        "            gradients_subset = gradients[sample_indices]\n",
        "            hessians_subset = hessians[sample_indices]\n",
        "\n",
        "            # Train a base learner (Decision Tree Regressor) using gradients and hessians on the subset\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth,\n",
        "                                         min_samples_split=self.min_samples_split, # Pass min_samples_split\n",
        "                                         reg_lambda=self.reg_lambda,\n",
        "                                         reg_gamma=self.reg_gamma)\n",
        "            tree.fit(X_train_subset, gradients_subset, hessians_subset)\n",
        "\n",
        "            # Make predictions with the new tree on the *full* training data\n",
        "            # Need to handle the selected features for prediction\n",
        "            tree_pred_train = tree.predict(X_train[:, feature_indices])\n",
        "\n",
        "            # Update the overall predictions on training data\n",
        "            y_pred_train += self.learning_rate * tree_pred_train\n",
        "\n",
        "            # Store the trained tree and the features used\n",
        "            self.estimators.append((tree, feature_indices))\n",
        "\n",
        "            # Evaluate on validation data and check for early stopping\n",
        "            if X_val is not None:\n",
        "                # Need to handle the selected features for validation prediction\n",
        "                tree_pred_val = tree.predict(X_val[:, feature_indices])\n",
        "                y_pred_val += self.learning_rate * tree_pred_val\n",
        "                val_score = self._eval_metric(y_val, y_pred_val)\n",
        "\n",
        "                if 'validation_0' not in self.evals_result:\n",
        "                    self.evals_result['validation_0'] = {self._eval_metric.__name__: []}\n",
        "                self.evals_result['validation_0'][self._eval_metric.__name__].append(val_score)\n",
        "\n",
        "\n",
        "                if val_score < best_val_score - self.tol: # Use tolerance for improvement\n",
        "                    best_val_score = val_score\n",
        "                    no_change_count = 0\n",
        "                else:\n",
        "                    no_change_count += 1\n",
        "\n",
        "                if self.n_iter_no_change is not None and no_change_count >= self.n_iter_no_change:\n",
        "                    print(f\"Early stopping at iteration {i+1}. Best validation score: {best_val_score:.4f}\")\n",
        "                    self.estimators = self.estimators[:i+1] # Keep only the best set of estimators\n",
        "                    break\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Makes predictions on new data.\"\"\"\n",
        "        if self.initial_prediction is None:\n",
        "            raise RuntimeError(\"Model has not been fitted yet.\")\n",
        "\n",
        "        # Start with the initial prediction\n",
        "        y_pred = np.full(len(X), self.initial_prediction, dtype=float)\n",
        "\n",
        "        # Add predictions from each subsequent tree, using the features they were trained on\n",
        "        for tree, feature_indices in self.estimators:\n",
        "            y_pred += self.learning_rate * tree.predict(X[:, feature_indices])\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "# Note on Objective Function: The `calculate_objective` function defined earlier\n",
        "# is useful for understanding the theory but is not directly used in the `fit` method\n",
        "# of the `GradientBoostingRegressor` class as implemented here. The `fit` method\n",
        "# minimizes the objective implicitly by training trees on gradients and using the\n",
        "# gain calculation during splitting, which is derived from the objective function."
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}